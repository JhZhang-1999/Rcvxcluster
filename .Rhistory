JADElr <- read.csv('result_instance2_learning.csv',sep=',',header=T)
JADElr <- JADElr[,2:4]
colnames(JADElr) <- c('AVG','MAX','MIN')
pp3 <- ggplot(data=NULL,aes(seq(1,50),JADElr)) +
geom_line(aes(seq(1,30),JADElr$MIN)) +
geom_line(aes(seq(1,30),JADElr$MAX)) +
geom_line(aes(seq(1,30),JADElr$AVG)) +
theme_classic() + xlab('Epoch') + ylab('Space Utilization') +
ggtitle('Learning Curve of JADE')
pp3
SAlr <- read.table('SA_ins2.txt',sep=',')
colnames(SAlr) <- c('FIT')
pp4 <- ggplot(data=NULL,aes(seq(1,15),SAlr)) +
geom_line(aes(seq(1,15),SAlr$FIT)) +
theme_classic() + xlab('Epoch') + ylab('Space Utilization') +
ggtitle('Learning Curve of SA')
pp4
pcom<-ggarrange(pp1,pp2,pp3,pp4,nrow=2, ncol=2)
pcom
ggsave('lr.png',width=12,height=10)
oriGAlr <- read.table('oriGAhomo_out.txt',sep=',')
oriGAlr <- oriGAlr[,1:4]
colnames(oriGAlr) <- c('Instance','AVG','MAX','MIN')
oriGAlr
pp1 <- ggplot(data=NULL,aes(seq(1,50),oriGAlr[oriGAlr$Instance==2,])) +
geom_line(aes(seq(1,50),oriGAlr[oriGAlr$Instance==2,]$MIN)) +
geom_line(aes(seq(1,50),oriGAlr[oriGAlr$Instance==2,]$MAX)) +
geom_line(aes(seq(1,50),oriGAlr[oriGAlr$Instance==2,]$AVG)) +
theme_classic() + xlab('Epoch') + ylab('Space Utilization') +
ggtitle('Learning Curve of Original GA') + ylim(0.3,1)
pp1
GATlr <- read.table('final_ins2.txt',sep=',')
GATlr <- GATlr[,2:4]
colnames(GATlr) <- c('AVG','MAX','MIN')
pp2 <- ggplot(data=NULL,aes(seq(1,50),GATlr)) +
geom_line(aes(seq(1,50),GATlr$MIN)) +
geom_line(aes(seq(1,50),GATlr$MAX)) +
geom_line(aes(seq(1,50),GATlr$AVG)) +
theme_classic() + xlab('Epoch') + ylab('Space Utilization') +
ggtitle('Learning Curve of GA Tree') + ylim(0.3,1)
pp2
JADElr <- read.csv('result_instance2_learning.csv',sep=',',header=T)
JADElr <- JADElr[,2:4]
colnames(JADElr) <- c('AVG','MAX','MIN')
pp3 <- ggplot(data=NULL,aes(seq(1,50),JADElr)) +
geom_line(aes(seq(1,30),JADElr$MIN)) +
geom_line(aes(seq(1,30),JADElr$MAX)) +
geom_line(aes(seq(1,30),JADElr$AVG)) +
theme_classic() + xlab('Epoch') + ylab('Space Utilization') +
ggtitle('Learning Curve of JADE') + ylim(0.3,1)
pp3
SAlr <- read.table('SA_ins2.txt',sep=',')
colnames(SAlr) <- c('FIT')
pp4 <- ggplot(data=NULL,aes(seq(1,15),SAlr)) +
geom_line(aes(seq(1,15),SAlr$FIT)) +
theme_classic() + xlab('Epoch') + ylab('Space Utilization') +
ggtitle('Learning Curve of SA') + ylim(0.3,1)
pp4
pcom<-ggarrange(pp1,pp2,pp3,pp4,nrow=2, ncol=2)
pcom
ggsave('lr2.png',width=12,height=10)
sqrt(2*400*240000/0.2/1)
sqrt(2*400*240000/0.2/.98)
400*240000/31298.43+.98*240000+.2*.98*31298.43/2
sqrt(2*400*240000/0.2/.96)
400*240000/20000+.98*240000+.2*.98*20000/2
400*240000/40000+.98*240000+.2*.98*40000/2
31298.43/
2
sqrt(2*400*240000/0.2/.96)
400*240000/31622.78+.96*240000+.2*.96*31622.78/2
sqrt(2*400*240000/0.2/.96)/2
x <- c(1,2)
y <- c(3,4)
res <- c(x,y)
res
20000+.98*20000
0.02*20000
39600-.96*40000
sqrt(2*400*240000/.2)
sqrt(2*800*240000/.2/.98)
sqrt(2*1600*240000/.2/.96)
0.96*240000+1200*240000/63245.55+400*240000/63245.55+0.2*0.96/2*63245.55+.2*600
sqrt(2*1600*240000/.2/.96)/2
library(devtools)
devtools::install_github('SugiharaLab/rEDM')
install.packages("D:/Application/CODING/R/rEDM_1.7.0.tar.gz", repos = NULL, type = "source")
download.file('https://cran.r-project.org/src/contrib/rEDM_1.7.0.tar.gz',
f <- tempfile())
unzip(f, exdir=tempdir())
download.file('https://cran.r-project.org/bin/windows/contrib/4.0/rEDM_1.7.0.zip',
f <- tempfile())
unzip(f, exdir=tempdir())
load(file.path(tempdir(), '.RData'))
install.packages('CausalImpact')
knitr::opts_chunk$set(echo = TRUE)
library(CausalImpact)
set.seed(1)
x1 <- 100 + arima.sim(model = list(ar = 0.999), n = 100)
y <- 1.2 * x1 + rnorm(100)
y[71:100] <- y[71:100] + 10
data <- cbind(y, x1)
matplot(data, type = "l")
pre.period <- c(1, 70)
post.period <- c(71, 100)
impact <- CausalImpact(data,pre.period,post.period)
plot(impact)
0.08*3/8+0.16*5/8
0.13+0.05*1/3
(0.13+0.05*1/3)*60
50*.16
.14+(.14-.095)*45/55
9.5%
0.095*0.6*0.45+0.1768*0.55
0.06*0.2+0.12*0.8
ratio <- seq(4,0.1,0.01)
ratio <- seq(4,0.1,-0.01)
ratio <- seq(0.25,4,0.01)
re <- 0.108+0.048*ratios
re <- 0.108+0.048*ratio
plot(ratio,re)
plot(ratio,re,type='l')
points(ratio,rep(0.108,length(ratio)),type='l')
points(ratio,rep(0.108,length(ratio)),type='l')
plot(ratio,re,type='l')
re
ratio
ratio <- seq(0,4,0.01)
re <- 0.108+0.048*ratios
re <- 0.108+0.048*ratio
plot(ratio,re)
plot(ratio,re,type='l')
points(ratio,0.108)
points(ratio,rep(0.108,length(ratio)),type='l')
plot(ratio,re,type='l',ylim=c(0.05,0.35))
points(ratio,rep(0.108,length(ratio)),type='l')
plot(ratio,re,type='l',ylim=c(0.05,0.35))
plot(ratio,re,type='l',ylim=c(0.05,0.35),col='red')
points(ratio,rep(0.108,length(ratio)),type='l',col='blue')
f <- function(x1,x2,x3){((1-0.2^x1)*(1-0.25^x2)*(1-0.3^x3))}
g <- function(x1,x2,x3){(4.5*x1+1.7*x2+2.3*x3)}
f(2.96104,4.02874,4.27237)
g(2.96104,4.02874,4.27237)
f(2,5.024,5.41705)
f(3,3.98837,4.22599)
f(3,3,4.95652)
f(3,4,4.21739)
g(3,4,4.21739)
f(3,4,4)
f(3,4,5)
g(3,4,4)
g(3,4,5)
6*3+4*9
6*3+5*4+3*4
1-exp(-0.5)/5/10^4
(1-exp(-0.5)/5/10^4)/0.5
(1-exp(-.5))/.5
(1-exp(-.5))/.5*0.98
(1-exp(-.5))/(5*10^(-4))+0.98*(1-exp(-10))/5/10^(-4)
2746.85/1200
(1-exp(-.5))/(5*10^(-4))+0.98*(1-exp(-.1))/5/10^(-4)
973.4573/1200
0.98*exp(-5*10^(-4)*(200))
setwd(r'D:\LIFE\WORK\4-University\Research\Rcvxcluster_cont\Rcvxcluster')
setwd('D:\\LIFE\\WORK\\4-University\\Research\\Rcvxcluster_cont\\Rcvxcluster')
library(devtools)
devtools::document()
devtools:check()
devtools::check()
library(expm)
help(expm)
devtools::document()
devtools::check()
devtools::document()
devtools::document()
devtools::check()
devtools::build()
install.packages('D:/LIFE/WORK/4-University/Research/Rcvxcluster_cont/Rcvxcluster_1.1.2.tar.gz', repos = NULL, type = "source")
library(Rcvxcluster)
help(Rcvxcluster)
help(Huber_HDMM)
help(Huber_ADMM)
help(positive_part)
help(weight_delta)
library(MASS)
library(clues)
library(cvxclustr)
mu1 <- mvrnorm(mu=rep(0,p),Sigma = diag(1,p))
X1 <- mvrnorm(N,mu=mu1,Sigma=diag(1,p))
mu2 <- mvrnorm(mu=rep(5,p),Sigma = diag(1,p))
X2 <- mvrnorm(N,mu=mu2,Sigma=diag(1,p))
X <- rbind(X1, X2)
set.seed(1234)
N=25
p=20
mu1 <- mvrnorm(mu=rep(0,p),Sigma = diag(1,p))
X1 <- mvrnorm(N,mu=mu1,Sigma=diag(1,p))
mu2 <- mvrnorm(mu=rep(5,p),Sigma = diag(1,p))
X2 <- mvrnorm(N,mu=mu2,Sigma=diag(1,p))
X <- rbind(X1, X2)
n = dim(X)[1]
X
X.shape()
length(X)
length(X[,1])
dim(X)
length(X[1,])
outliers <- sample(c(runif(n=5,min=20,max=50),runif(n=5,min=-50,max=-20)))
samind <- sample(1:(n*p),10)
X[samind] <- outliers
outliers
wt <- weight_delta(X)
wt
dim(wt)
H <- Huber_ADMM(X,lam=0.7,wt=wt)
lowerTriangle(wt)
lower.tri(wt)
library(gdata)
lowerTriangle(wt)
dim(wt)
dim(lowerTriangle(wt))
length(lowerTriangle(wt))
library(MASS)
library(clues)
library(cvxclustr)
set.seed(1234) # maybe should be a variable later
N=25 #?
p=20 #?
# mvrnorm: multivariate normal distribution
mu1 <- mvrnorm(mu=rep(0,p),Sigma = diag(1,p))
X1 <- mvrnorm(N,mu=mu1,Sigma=diag(1,p))
mu2 <- mvrnorm(mu=rep(5,p),Sigma = diag(1,p))
X2 <- mvrnorm(N,mu=mu2,Sigma=diag(1,p))
X <- rbind(X1, X2) # dim(X) = 2*N p
n = dim(X)[1] # why not n=2*N directly?
outliers <- sample(c(runif(n=5,min=20,max=50),runif(n=5,min=-50,max=-20)))
X[sample(1:(n*p),10)] <- outliers # put outliers randomly in X
### Weights
# create wt using 2.5
distance <- matrix(0,nrow=n,ncol=n)
for(i in seq(from=1,to=n-1,by=1)){
for(j in seq(from=i+1,to=n, by=1)){
distance[i,j]<-norm(X[i,] - X[j,],type = "2")
distance[j,i]<-norm(X[i,] - X[j,],type = "2")
}
}
delta <- 15 # variable later?
zeta <- 0.1 # variable later?
wt <- matrix(0,n,n) # n*n matrix, all elements=0
for(i in 1:n){
for(j in 1:n){
if(distance[i,j]==0){
wt[i,j]=0
}
else if(distance[i,j]<delta){
wt[i,j]=exp(-0.5*zeta*distance[i,j]) # why 0.5?
}
else{wt[i,j]=exp(-0.5*zeta*delta)} # why 0.5?
}
}
library(gdata)
wt.vec <- lowerTriangle(wt) # the lowerTri part of wt, as a vector(bycol)
H <- Huber_ADMM(X,lam=0.7,wt=wt.vec) # using Huber function
A <- create_adjacency(t(H$V),wt.vec,n,method="admm") # QUESTION HERE
cl2 <- find_clusters(A)$cluster # QUESTION HERE
### Using Eric Chi's Package [cvxcluster]
H_2 <- create_clustering_problem(p,n,method='admm',seed=1234)
ix <- H_2$ix
M1 <- H_2$M1
M2 <- H_2$M2
s1 <- H_2$s1
s2 <- H_2$s2
k = length(wt1.vec)
Lambda <- matrix(rnorm(p*k),p,k)
max_iter <- 1e5
tol_abs <- 1e-15
tol_rel <- 1e-15
# create wt1
nn=5 # the number of nearest neighbors # QUESTION HERE, is this a variable?
wt1 <- distance # why initialize wt1 to distance, while it will be reset to 0 afterwhile?
for(i in 1:n){
index <- order(wt1[,i],decreasing = FALSE)[1:(nn+1)]
wt1[,i] <- rep(0,n)
wt1[index,i] <- rep(1,length(index))
}
diag(wt1) <- 0
wt1.vec <- lowerTriangle(wt1)
# create wt2
wt2 <- wt1*exp(-0.5*distance)
wt2.vec <- lowerTriangle(wt2)
## wt1.vec stands for phi=0 # QUESTION HERE
example_result1 <- cvxclust_admm(X=t(X),Lambda=Lambda,ix=ix,M1=M1,M2=M2,s1=s1,s2=s2,
w=wt1.vec,gamma=5,nu=1,accelerate=FALSE,
max_iter=max_iter,tol_abs=tol_abs,
tol_rel=tol_rel)
## wt2.vec stands for phi=0.5 # QUESTION HERE
example_result2 <- cvxclust_admm(X=t(X),Lambda=Lambda,ix=ix,M1=M1,M2=M2,s1=s1,s2=s2,
w=wt2.vec,gamma=5,nu=1,accelerate=FALSE,
max_iter=max_iter,
tol_abs=tol_abs,tol_rel=tol_rel)
## Construct adjacency matrix
B <- create_adjacency(example_result1$V,wt1.vec,n,method = "admm")
C <- create_adjacency(example_result2$V,wt2.vec,n,method = "admm")
cl3 <- find_clusters(B)$cluster
cl4 <- find_clusters(C)$cluster
cl_true <- c(rep(1,25),rep(2,25))
# Using huber loss
adjustedRand(cl_true,cl2)
# Using Eric Chi's Function
adjustedRand(cl_true,cl3) # phi=0
adjustedRand(cl_true,cl4) # phi=0.5
# how to present the comparison results?
help("cvxclust_admm")
library(MASS)
library(clues)
library(cvxclustr)
set.seed(1234) # maybe should be a variable later
N=25 #?
p=20 #?
# mvrnorm: multivariate normal distribution
mu1 <- mvrnorm(mu=rep(0,p),Sigma = diag(1,p))
X1 <- mvrnorm(N,mu=mu1,Sigma=diag(1,p))
mu2 <- mvrnorm(mu=rep(5,p),Sigma = diag(1,p))
X2 <- mvrnorm(N,mu=mu2,Sigma=diag(1,p))
X <- rbind(X1, X2) # dim(X) = 2*N p
n = dim(X)[1] # why not n=2*N directly?
outliers <- sample(c(runif(n=5,min=20,max=50),runif(n=5,min=-50,max=-20)))
X[sample(1:(n*p),10)] <- outliers # put outliers randomly in X
### Weights
# create wt using 2.5
distance <- matrix(0,nrow=n,ncol=n)
for(i in seq(from=1,to=n-1,by=1)){
for(j in seq(from=i+1,to=n, by=1)){
distance[i,j]<-norm(X[i,] - X[j,],type = "2")
distance[j,i]<-norm(X[i,] - X[j,],type = "2")
}
}
delta <- 15 # variable later?
zeta <- 0.1 # variable later?
wt <- matrix(0,n,n) # n*n matrix, all elements=0
for(i in 1:n){
for(j in 1:n){
if(distance[i,j]==0){
wt[i,j]=0
}
else if(distance[i,j]<delta){
wt[i,j]=exp(-0.5*zeta*distance[i,j]) # why 0.5?
}
else{wt[i,j]=exp(-0.5*zeta*delta)} # why 0.5?
}
}
library(gdata)
wt1.vec <- lowerTriangle(wt) # the lowerTri part of wt, as a vector(bycol)
H <- Huber_ADMM(X,lam=0.7,wt=wt.vec) # using Huber function
A <- create_adjacency(t(H$V),wt.vec,n,method="admm") # QUESTION HERE
cl2 <- find_clusters(A)$cluster # QUESTION HERE
### Using Eric Chi's Package [cvxcluster]
H_2 <- create_clustering_problem(p,n,method='admm',seed=1234)
ix <- H_2$ix
M1 <- H_2$M1
M2 <- H_2$M2
s1 <- H_2$s1
s2 <- H_2$s2
k = length(wt1.vec)
Lambda <- matrix(rnorm(p*k),p,k)
max_iter <- 1e5
tol_abs <- 1e-15
tol_rel <- 1e-15
# create wt1
nn=5 # the number of nearest neighbors # QUESTION HERE, is this a variable?
wt1 <- distance # why initialize wt1 to distance, while it will be reset to 0 afterwhile?
for(i in 1:n){
index <- order(wt1[,i],decreasing = FALSE)[1:(nn+1)]
wt1[,i] <- rep(0,n)
wt1[index,i] <- rep(1,length(index))
}
diag(wt1) <- 0
wt1.vec <- lowerTriangle(wt1)
# create wt2
wt2 <- wt1*exp(-0.5*distance)
wt2.vec <- lowerTriangle(wt2)
Lambda <-
## wt1.vec stands for phi=0 # QUESTION HERE
example_result1 <- cvxclust_admm(X=t(X),Lambda=Lambda,ix=ix,M1=M1,M2=M2,s1=s1,s2=s2,
w=wt1.vec,gamma=5,nu=1,accelerate=FALSE,
max_iter=max_iter,tol_abs=tol_abs,
tol_rel=tol_rel)
## wt2.vec stands for phi=0.5 # QUESTION HERE
example_result2 <- cvxclust_admm(X=t(X),Lambda=Lambda,ix=ix,M1=M1,M2=M2,s1=s1,s2=s2,
w=wt2.vec,gamma=5,nu=1,accelerate=FALSE,
max_iter=max_iter,
tol_abs=tol_abs,tol_rel=tol_rel)
## Construct adjacency matrix
B <- create_adjacency(example_result1$V,wt1.vec,n,method = "admm")
C <- create_adjacency(example_result2$V,wt2.vec,n,method = "admm")
cl3 <- find_clusters(B)$cluster
cl4 <- find_clusters(C)$cluster
cl_true <- c(rep(1,25),rep(2,25))
# Using huber loss
adjustedRand(cl_true,cl2)
# Using Eric Chi's Function
adjustedRand(cl_true,cl3) # phi=0
adjustedRand(cl_true,cl4) # phi=0.5
# how to present the comparison results?
library(MASS)
library(clues)
library(cvxclustr)
set.seed(1234)
N=25
p=20
mu1 <- mvrnorm(mu=rep(0,p),Sigma = diag(1,p))
X1 <- mvrnorm(N,mu=mu1,Sigma=diag(1,p))
mu2<-mvrnorm(mu=rep(5,p),Sigma = diag(1,p))
X2<-mvrnorm(N,mu=mu2,Sigma=diag(1,p))
X <- rbind(X1, X2)
n=dim(X)[1]
outliers<-sample(c(runif(n=5,min = 20,max = 50),runif(n=5,min=-50,max=-20)))
X[sample(1:(n*p),10)] <- outliers
### Weights
distance<-matrix(0,nrow=n,ncol=n)
for(i in seq(from=1,to=n-1,by=1)){
for(j in seq(from=i+1,to=n, by=1)){
distance[i,j]<-norm(X[i,]-X[j,],type = "2")
distance[j,i]<-norm(X[i,]-X[j,],type = "2")
}
}
delta <- 15
zeta <- 0.1
wt<-matrix(0,n,n)
for(i in 1:n){
for(j in 1:n){
if(distance[i,j]==0){
wt[i,j]=0}else if(distance[i,j]<delta){
wt[i,j]=exp(-0.5*zeta*distance[i,j])
}else{wt[i,j]=exp(-0.5*zeta*delta)}
}
}
library(gdata)
wt.vec<-lowerTriangle(wt)
H <- Huber_ADMM(X,lam=0.7,wt=wt.vec)
A <- create_adjacency(t(H$V),wt.vec,n,method = "admm")
cl2 <- find_clusters(A)$cluster
## Using Eric Chi's Package
H_2<-create_clustering_problem(p,n,method='admm',seed=1234)
ix<-H_2$ix
M1<-H_2$M1
M2<-H_2$M2
s1 <- H_2$s1
s2 <- H_2$s2
k=length(wt1.vec)
Lambda <- matrix(rnorm(p*k),p,k)
max_iter <- 1e5
tol_abs <- 1e-15
tol_rel <- 1e-15
# create wt1
nn=5 #the number of nearest neighbors
wt1<-distance
for(i in 1:n){
index<-order(wt1[,i],decreasing = FALSE)[1:(nn+1)]
wt1[,i]<-rep(0,n)
wt1[index,i]<-rep(1,length(index))
}
diag(wt1) <- 0
wt1.vec <- lowerTriangle(wt1)
# create wt2
wt2<-wt1*exp(-0.5*distance)
wt2.vec<-lowerTriangle(wt2)
## wt1.vec stands for phi=0
example_result1<-cvxclust_admm(X=t(X),Lambda = Lambda,ix=ix,M1=M1,M2=M2,s1=s1,s2=s2,
w=wt1.vec,gamma = 5,nu=1,accelerate = FALSE,
max_iter=max_iter,tol_abs=tol_abs,
tol_rel=tol_rel)
## wt2.vec stands for phi=0.5
example_result2<-cvxclust_admm(X=t(X),Lambda = Lambda,ix=ix,M1=M1,M2=M2,s1=s1,s2=s2,
w=wt2.vec,gamma = 5,nu=1,accelerate = FALSE,
max_iter=max_iter,
tol_abs=tol_abs,tol_rel=tol_rel)
## Construct adjacency matrix
B <- create_adjacency(example_result1$V,wt1.vec,n,method = "admm")
C <- create_adjacency(example_result2$V,wt2.vec,n,method = "admm")
cl3 <- find_clusters(B)$cluster
cl4 <- find_clusters(C)$cluster
cl_true<-c(rep(1,25),rep(2,25))
# Using huber loss
adjustedRand(cl_true,cl2)
# Using Eric Chi's Function
adjustedRand(cl_true,cl3)
adjustedRand(cl_true,cl4)
library(plot3D)
index1 <- c(1:(n*p))
scatter3D(index1, index1, X, phi=0, bty="b2")
dim(X)
scatter3D(index1, index1, X[1:25,:], phi=0, bty="b2")
scatter3D(index1, index1, X[1:25,], phi=0, bty="b2")
scatter3D(index1[1:25,], index1[1:25,], X[1:25,], phi=0, bty="b2")
scatter3D(index1[1:25], index1[1:25], X[1:25,], phi=0, bty="b2")
X[1:25]
X[1:25,]
dim(X[1:25,])
index1
index2 <- c(1:(25*p))
scatter3D(index2, index2, X[1:25,], phi=0, bty="b2")
scatter3D(index2, index2, X[26:50,], phi=0, bty="b2")
help("scatter3D")
scatter3D(c(1,2,3),c(5,2,7),c(10,10,10), phi=0, bty="b2")
length(X[1:25,])
pca <- princomp(X)
newX<-as.data.frame(pca$scores[,1:3])
dim(newX)
scatter3D(X[,1],X[,2],X[,3], phi=0, bty="b2")
scatter3D(X[1:25,1],X[1:25,2],X[1:25,3], phi=0, bty="b2")
scatter3D(X[26:50,1],X[26:50,2],X[26:50,3], phi=0, bty="b2")
